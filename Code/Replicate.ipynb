{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import replicate\n",
    "import os\n",
    "import pandas as pd\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] =\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(model):\n",
    "    model_url = \"\"\n",
    "    if model == \"Llama-7b\":\n",
    "        model_url = \"meta/llama-2-7b-chat:8e6975e5ed6174911a6ff3d60540dfd4844201974602551e10e9e87ab143d81e\"\n",
    "    elif model == \"Llama-13b\":\n",
    "        model_url = \"meta/llama-2-13b-chat:f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d\"\n",
    "    elif model == \"Llama-70b\":\n",
    "        model_url = \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\"\n",
    "    elif model == \"Mistral-7b\":\n",
    "        model_url = \"a16z-infra/mistral-7b-instruct-v0.1:83b6a56e7c828e667f21fd596c338fd4f0039b46bcfa18d973e8e70e455fda70\"\n",
    "    else:\n",
    "        model_url = \"ERROR\"\n",
    "\n",
    "    return model_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automate_llama(question, model):\n",
    "    output = replicate.run(\n",
    "        select_model(model),\n",
    "        input={\"prompt\": question,\n",
    "               \"temperature\": 0.1, \"max_tokens\": 1000,}\n",
    "    )\n",
    "    final_output = \"\"\n",
    "    for item in output:\n",
    "        final_output += item\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_create_columns(df, column_names):\n",
    "    for column_name in column_names:\n",
    "        if column_name not in df.columns:\n",
    "            df[column_name] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_experiment(df):\n",
    "\n",
    "    for index in range(len(df)):\n",
    "        question = df.loc[index, 'question']\n",
    "        print(index)\n",
    "        for model in [\"Llama-7b\", \"Llama-13b\", \"Llama-70b\", \"Mistral-7b\"]:\n",
    "            df.loc[index, f\"Propmt-{model}\"] = automate_llama(question, model)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'toefl_questions'\n",
    "\n",
    "df = pd.read_excel(f\"Datasets/{file_name}.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_experiment(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(f\"Datasets/{file_name}_result.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
